{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeara\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import scripts\n",
    "from scripts.saveResults import  *\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "# This ensures Reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SolarRadiance(Dataset):\n",
    "    def __init__(self, root_dir, labels, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        # self.data = self.load_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.labels.iloc[index,0]\n",
    "        target = self.labels.iloc[index,1]\n",
    "\n",
    "        ''' While using the traditional upscaling methods\n",
    "            the following image processing codes are applied.\n",
    "            The upscaling itself is perfromed here'''\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize the image\n",
    "        # fx and fy values determine the scaling ratio, 4 means 4x upscaling is applied\n",
    "        # interpolation methods can be cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC\n",
    "        image = cv2.resize(image, None, fx = 4, fy = 4, interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "        # Normalize the image\n",
    "        image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "\n",
    "        ''' While using the deep learning based upscaling methods\n",
    "            the following image processing code is applied.\n",
    "            The images are only read and normalized '''\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # Normalize the image\n",
    "        image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "             image = self.transform(image)\n",
    "\n",
    "        y_label = torch.tensor(target)\n",
    "\n",
    "        return image, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root_dir):\n",
    "        ds = pd.DataFrame()\n",
    "        dates = os.listdir(root_dir)\n",
    "\n",
    "        try:\n",
    "            for date in dates:\n",
    "                infrared_folder = os.path.join(root_dir, date, \"infrared\")\n",
    "                pyranometer_folder = os.path.join(root_dir, date, \"pyranometer\")\n",
    "                csv_path = os.path.join(pyranometer_folder, \"{date}.csv\".format(date=date))\n",
    "                if not os.path.exists(csv_path):\n",
    "                    print(\"Skipping date {date} because it does not have both infrared and pyranometer folders\".format(date=date))\n",
    "                    continue\n",
    "                ds_temp = getDs(infrared_folder, csv_path)\n",
    "                \n",
    "                # append the dataframe in the final dataframe\n",
    "                # ds = ds.append(ds_temp)\n",
    "                ds = pd.concat([ds, ds_temp], ignore_index=True)\n",
    "\n",
    "                ds['name'] = ds['name'].apply(lambda img: os.path.join(root_dir, date, 'infrared', img))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return ds\n",
    "\n",
    "def getDs(path, labels):\n",
    "    pyranometer = pd.read_csv(labels)\n",
    "    images = os.listdir(path)\n",
    "    \n",
    "   \n",
    "    #convert column 1 to int\n",
    "    X = pyranometer.iloc[:,0].astype(int)\n",
    "\n",
    "    #convert to image names\n",
    "    pyranometer.iloc[:,0] = X.apply(lambda x: str(x) + 'IR.png')\n",
    "    \n",
    "    # Filter pyranometer DataFrame based on the 'x' column\n",
    "\n",
    "    filtered_pyranometer = pyranometer[pyranometer.iloc[:,0].isin(images)]\n",
    "    # Display the result\n",
    "    filtered_pyranometer.columns = ['name', 'value']\n",
    "\n",
    "    filtered_pyranometer = filtered_pyranometer.drop_duplicates(subset='name')\n",
    "    return filtered_pyranometer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 1\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "loss = 1 # if loss = 0 the model will be trained with RMSE loss and vice versa\n",
    "lr=0.01 # learning rate\n",
    "result_dir = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/yeara/OneDrive/Desktop/IR Upscaling/datasets/GIRASOL Dataset Extracted/train/'\n",
    "train_data  = load_dataset(train_dir)\n",
    "train_set = SolarRadiance(root_dir= train_dir, labels=train_data, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dir = 'C:/Users/yeara/OneDrive/Desktop/IR Upscaling/datasets/GIRASOL Dataset Extracted/val/'\n",
    "val_data  = load_dataset(val_dir)\n",
    "val_set = SolarRadiance(root_dir= val_dir, labels=val_data, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46527\n",
      "11574\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset))\n",
    "print(len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNRegression(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CNNRegression(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = CNNRegression()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss == 0:\n",
    "    # create a function (this my favorite choice)\n",
    "    def RMSELoss(yhat,y):\n",
    "        return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "    criterion = RMSELoss\n",
    "else:\n",
    "    # Define the model, loss function, and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Create a StepLR scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "# set initial loswest_loss to an infinite number\n",
    "lowest_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    print('Epoch ',epoch)\n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs[:,0], targets.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs[:,0], targets.float())\n",
    "            total_loss += loss.item()\n",
    "        mean_loss = total_loss / len(val_loader)\n",
    "        print(f'val Loss: {mean_loss:.4f}')\n",
    "        losses.append(mean_loss)\n",
    "        if  mean_loss<lowest_loss:\n",
    "            print('mean_loss: '+str(mean_loss)+' lowest_loss: '+str(lowest_loss))\n",
    "            lowest_loss = mean_loss\n",
    "            torch.save(model.state_dict(), os.path.join(result_dir,'model-save.pth'))\n",
    "            print('---saved a model due to lower loss---')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
